{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c6869d9",
   "metadata": {},
   "source": [
    "# Optimización de Hiperparámetros en Machine Learning\n",
    "\n",
    "Este notebook explora diferentes técnicas de optimización de hiperparámetros usando el dataset del Titanic:\n",
    "1. GridSearchCV\n",
    "2. RandomizedSearchCV\n",
    "3. Optimización Bayesiana (con scikit-optimize)\n",
    "4. Optuna\n",
    "\n",
    "## Preparación de datos y configuración inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75b82abf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Importamos las bibliotecas necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16604094",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Cargamos y preparamos los datos del Titanic\n",
    "def preparar_datos_titanic():\n",
    "    # Cargamos los datos\n",
    "    df = pd.read_csv('https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv')\n",
    "\n",
    "    # Preprocesamiento básico\n",
    "    df['Age'].fillna(df['Age'].median(), inplace=True)\n",
    "    df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "    # Codificación de variables categóricas\n",
    "    df['Sex'] = df['Sex'].map({'female': 1, 'male': 0})\n",
    "    embarked_dummies = pd.get_dummies(df['Embarked'], prefix='Embarked')\n",
    "    df = pd.concat([df, embarked_dummies], axis=1)\n",
    "\n",
    "    # Seleccionamos características\n",
    "    features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare',\n",
    "               'Embarked_C', 'Embarked_Q', 'Embarked_S']\n",
    "    X = df[features]\n",
    "    y = df['Survived']\n",
    "\n",
    "    # Dividimos los datos\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Escalamos las características\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b960bb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datos\n",
    "X_train, X_test, y_train, y_test = preparar_datos_titanic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d856de",
   "metadata": {},
   "source": [
    "## 1. GridSearchCV\n",
    "Búsqueda exhaustiva en una cuadrícula de hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a01957b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros (Grid): {'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Mejor puntuación (Grid): 0.8314192849404117\n",
      "\n",
      "Informe de clasificación (Grid):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.90      0.86       105\n",
      "           1       0.84      0.72      0.77        74\n",
      "\n",
      "    accuracy                           0.83       179\n",
      "   macro avg       0.83      0.81      0.82       179\n",
      "weighted avg       0.83      0.83      0.82       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definimos el espacio de búsqueda\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],   # el primer numero y el none gay que ponerlo\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Creamos y ejecutamos GridSearchCV\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Evaluamos los resultados\n",
    "print(\"Mejores parámetros (Grid):\", grid_search.best_params_)\n",
    "print(\"Mejor puntuación (Grid):\", grid_search.best_score_)\n",
    "y_pred_grid = grid_search.predict(X_test)\n",
    "print(\"\\nInforme de clasificación (Grid):\")\n",
    "print(classification_report(y_test, y_pred_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aec54c",
   "metadata": {},
   "source": [
    "## 2. RandomizedSearchCV\n",
    "Búsqueda aleatoria en el espacio de hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6512a1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros (Random): {'max_depth': 28, 'min_samples_leaf': 7, 'min_samples_split': 12, 'n_estimators': 187}\n",
      "Mejor puntuación (Random): 0.8286319314488327\n",
      "\n",
      "Informe de clasificación (Random):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.90      0.86       105\n",
      "           1       0.84      0.70      0.76        74\n",
      "\n",
      "    accuracy                           0.82       179\n",
      "   macro avg       0.83      0.80      0.81       179\n",
      "weighted avg       0.82      0.82      0.82       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Definimos distribuciones para los parámetros\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(100, 500),\n",
    "    'max_depth': randint(10, 50),\n",
    "    'min_samples_split': randint(2, 15),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "# Creamos y ejecutamos RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf, param_distributions, n_iter=20, cv=5,\n",
    "    scoring='accuracy', n_jobs=-1, random_state=42\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Evaluamos los resultados\n",
    "print(\"Mejores parámetros (Random):\", random_search.best_params_)\n",
    "print(\"Mejor puntuación (Random):\", random_search.best_score_)\n",
    "y_pred_random = random_search.predict(X_test)\n",
    "print(\"\\nInforme de clasificación (Random):\")\n",
    "print(classification_report(y_test, y_pred_random))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f266c22",
   "metadata": {},
   "source": [
    "## 3. Optimización Bayesiana\n",
    "Usando scikit-optimize para búsqueda más eficiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43b936f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros (Bayes): OrderedDict({'max_depth': 41, 'min_samples_leaf': 6, 'min_samples_split': 5, 'n_estimators': 500})\n",
      "Mejor puntuación (Bayes): 0.8272037821333595\n",
      "\n",
      "Informe de clasificación (Bayes):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.90      0.86       105\n",
      "           1       0.84      0.70      0.76        74\n",
      "\n",
      "    accuracy                           0.82       179\n",
      "   macro avg       0.83      0.80      0.81       179\n",
      "weighted avg       0.82      0.82      0.82       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Real\n",
    "\n",
    "# Definimos el espacio de búsqueda\n",
    "bayes_space = {\n",
    "    'n_estimators': Integer(100, 500),\n",
    "    'max_depth': Integer(10, 50),\n",
    "    'min_samples_split': Integer(2, 15),\n",
    "    'min_samples_leaf': Integer(1, 10)\n",
    "}\n",
    "\n",
    "# Creamos y ejecutamos BayesSearchCV\n",
    "bayes_search = BayesSearchCV(\n",
    "    rf, bayes_space, n_iter=20, cv=5,\n",
    "    scoring='accuracy', n_jobs=-1, random_state=42\n",
    ")\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "# Evaluamos los resultados\n",
    "print(\"Mejores parámetros (Bayes):\", bayes_search.best_params_)\n",
    "print(\"Mejor puntuación (Bayes):\", bayes_search.best_score_)\n",
    "y_pred_bayes = bayes_search.predict(X_test)\n",
    "print(\"\\nInforme de clasificación (Bayes):\")\n",
    "print(classification_report(y_test, y_pred_bayes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c8446b",
   "metadata": {},
   "source": [
    "## Apuntes mios\n",
    "- el grid y el bayesiano van a comporbar todos y dará el mejor, sin embargo, el random elige el mejor de los que tiene aleatoriamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80da5768",
   "metadata": {},
   "source": [
    "## 4. Optuna\n",
    "Framework de optimización automática de hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc9394a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-08 13:40:31,362] A new study created in memory with name: no-name-1f07f907-b992-4733-a306-62afc55d8e9a\n",
      "[I 2025-02-08 13:40:37,810] Trial 0 finished with value: 0.821609376538954 and parameters: {'n_estimators': 368, 'max_depth': 16, 'min_samples_split': 13, 'min_samples_leaf': 8}. Best is trial 0 with value: 0.821609376538954.\n",
      "[I 2025-02-08 13:40:41,052] Trial 1 finished with value: 0.8215699793164581 and parameters: {'n_estimators': 381, 'max_depth': 16, 'min_samples_split': 12, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.821609376538954.\n",
      "[I 2025-02-08 13:40:44,001] Trial 2 finished with value: 0.8145671230178273 and parameters: {'n_estimators': 374, 'max_depth': 32, 'min_samples_split': 6, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.821609376538954.\n",
      "[I 2025-02-08 13:40:44,843] Trial 3 finished with value: 0.8258150300403821 and parameters: {'n_estimators': 103, 'max_depth': 45, 'min_samples_split': 2, 'min_samples_leaf': 10}. Best is trial 3 with value: 0.8258150300403821.\n",
      "[I 2025-02-08 13:40:46,953] Trial 4 finished with value: 0.8229882793263075 and parameters: {'n_estimators': 298, 'max_depth': 34, 'min_samples_split': 11, 'min_samples_leaf': 9}. Best is trial 3 with value: 0.8258150300403821.\n",
      "[I 2025-02-08 13:40:50,215] Trial 5 finished with value: 0.8187826258248794 and parameters: {'n_estimators': 380, 'max_depth': 13, 'min_samples_split': 11, 'min_samples_leaf': 5}. Best is trial 3 with value: 0.8258150300403821.\n",
      "[I 2025-02-08 13:40:52,948] Trial 6 finished with value: 0.8271939328277357 and parameters: {'n_estimators': 294, 'max_depth': 27, 'min_samples_split': 5, 'min_samples_leaf': 6}. Best is trial 6 with value: 0.8271939328277357.\n",
      "[I 2025-02-08 13:40:55,522] Trial 7 finished with value: 0.8159952723333005 and parameters: {'n_estimators': 238, 'max_depth': 37, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 6 with value: 0.8271939328277357.\n",
      "[I 2025-02-08 13:41:01,170] Trial 8 finished with value: 0.8215699793164581 and parameters: {'n_estimators': 492, 'max_depth': 46, 'min_samples_split': 12, 'min_samples_leaf': 2}. Best is trial 6 with value: 0.8271939328277357.\n",
      "[I 2025-02-08 13:41:05,235] Trial 9 finished with value: 0.8202009258347289 and parameters: {'n_estimators': 317, 'max_depth': 41, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 6 with value: 0.8271939328277357.\n",
      "[I 2025-02-08 13:41:07,850] Trial 10 finished with value: 0.8300305328474342 and parameters: {'n_estimators': 185, 'max_depth': 24, 'min_samples_split': 3, 'min_samples_leaf': 6}. Best is trial 10 with value: 0.8300305328474342.\n",
      "[I 2025-02-08 13:41:09,589] Trial 11 finished with value: 0.8258150300403821 and parameters: {'n_estimators': 157, 'max_depth': 24, 'min_samples_split': 3, 'min_samples_leaf': 6}. Best is trial 10 with value: 0.8300305328474342.\n",
      "[I 2025-02-08 13:41:11,349] Trial 12 finished with value: 0.8243967300305328 and parameters: {'n_estimators': 225, 'max_depth': 25, 'min_samples_split': 4, 'min_samples_leaf': 6}. Best is trial 10 with value: 0.8300305328474342.\n",
      "[I 2025-02-08 13:41:12,906] Trial 13 finished with value: 0.8244065793361568 and parameters: {'n_estimators': 200, 'max_depth': 25, 'min_samples_split': 5, 'min_samples_leaf': 5}. Best is trial 10 with value: 0.8300305328474342.\n",
      "[I 2025-02-08 13:41:16,024] Trial 14 finished with value: 0.8215896779277061 and parameters: {'n_estimators': 284, 'max_depth': 28, 'min_samples_split': 8, 'min_samples_leaf': 7}. Best is trial 10 with value: 0.8300305328474342.\n",
      "[I 2025-02-08 13:41:18,296] Trial 15 finished with value: 0.8159854230276766 and parameters: {'n_estimators': 137, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 10 with value: 0.8300305328474342.\n",
      "[I 2025-02-08 13:41:22,615] Trial 16 finished with value: 0.8187727765192554 and parameters: {'n_estimators': 474, 'max_depth': 21, 'min_samples_split': 15, 'min_samples_leaf': 7}. Best is trial 10 with value: 0.8300305328474342.\n",
      "[I 2025-02-08 13:41:24,660] Trial 17 finished with value: 0.8173840244262781 and parameters: {'n_estimators': 261, 'max_depth': 30, 'min_samples_split': 5, 'min_samples_leaf': 4}. Best is trial 10 with value: 0.8300305328474342.\n",
      "[I 2025-02-08 13:41:26,115] Trial 18 finished with value: 0.8201910765291046 and parameters: {'n_estimators': 186, 'max_depth': 11, 'min_samples_split': 4, 'min_samples_leaf': 8}. Best is trial 10 with value: 0.8300305328474342.\n",
      "[I 2025-02-08 13:41:29,407] Trial 19 finished with value: 0.8187924751305033 and parameters: {'n_estimators': 439, 'max_depth': 37, 'min_samples_split': 9, 'min_samples_leaf': 7}. Best is trial 10 with value: 0.8300305328474342.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros (Optuna): {'n_estimators': 185, 'max_depth': 24, 'min_samples_split': 3, 'min_samples_leaf': 6}\n",
      "Mejor puntuación (Optuna): 0.8300305328474342\n",
      "\n",
      "Informe de clasificación (Optuna):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.90      0.86       105\n",
      "           1       0.84      0.70      0.76        74\n",
      "\n",
      "    accuracy                           0.82       179\n",
      "   macro avg       0.83      0.80      0.81       179\n",
      "weighted avg       0.82      0.82      0.82       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "def objetivo(trial):\n",
    "    # Definimos el espacio de búsqueda\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 10, 50),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 15),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "    }\n",
    "\n",
    "    # Creamos y entrenamos el modelo\n",
    "    rf = RandomForestClassifier(**params, random_state=42)\n",
    "\n",
    "    # Evaluamos usando validación cruzada\n",
    "    scores = cross_val_score(rf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    return scores.mean()\n",
    "\n",
    "# Creamos y ejecutamos el estudio de Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objetivo, n_trials=20)\n",
    "\n",
    "# Evaluamos los resultados\n",
    "print(\"Mejores parámetros (Optuna):\", study.best_params)\n",
    "print(\"Mejor puntuación (Optuna):\", study.best_value)\n",
    "\n",
    "# Entrenamos el modelo final con los mejores parámetros\n",
    "rf_final = RandomForestClassifier(**study.best_params, random_state=42)\n",
    "rf_final.fit(X_train, y_train)\n",
    "y_pred_optuna = rf_final.predict(X_test)\n",
    "print(\"\\nInforme de clasificación (Optuna):\")\n",
    "print(classification_report(y_test, y_pred_optuna))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ba44be",
   "metadata": {},
   "source": [
    "## Comparación de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a21d102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparación de resultados:\n",
      "               Método  Accuracy\n",
      "0        GridSearchCV  0.826816\n",
      "1  RandomizedSearchCV  0.821229\n",
      "2       BayesSearchCV  0.821229\n",
      "3              Optuna  0.821229\n"
     ]
    }
   ],
   "source": [
    "# Creamos un DataFrame con los resultados\n",
    "resultados = pd.DataFrame({\n",
    "    'Método': ['GridSearchCV', 'RandomizedSearchCV', 'BayesSearchCV', 'Optuna'],\n",
    "    'Accuracy': [\n",
    "        accuracy_score(y_test, y_pred_grid),\n",
    "        accuracy_score(y_test, y_pred_random),\n",
    "        accuracy_score(y_test, y_pred_bayes),\n",
    "        accuracy_score(y_test, y_pred_optuna)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Comparación de resultados:\")\n",
    "print(resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe87d74",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "Este notebook demuestra diferentes métodos de optimización de hiperparámetros:\n",
    "\n",
    "1. GridSearchCV: Búsqueda exhaustiva pero computacionalmente costosa\n",
    "2. RandomizedSearchCV: Más eficiente que Grid Search, buenos resultados con menos tiempo\n",
    "3. Optimización Bayesiana: Búsqueda inteligente que aprende de iteraciones anteriores\n",
    "4. Optuna: Framework moderno con características avanzadas y visualización\n",
    "\n",
    "Cada método tiene sus ventajas y desventajas en términos de tiempo de computación y calidad de resultados."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
