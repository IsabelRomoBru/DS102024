{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo ETL con Polars: Dataset de Taxis de Nueva York\n",
    "\n",
    "En esta sección, implementaremos un ejemplo completo de ETL (Extracción, Transformación y Carga) utilizando Polars para procesar el dataset de taxis de Nueva York. Este ejemplo demostrará las ventajas de Polars sobre Pandas en términos de rendimiento y funcionalidades.\n",
    "\n",
    "Nuestro ETL incluirá:\n",
    "1. Extracción de datos desde archivos Parquet\n",
    "2. Transformación y limpieza de datos con Polars\n",
    "3. Validación de datos con Pydantic\n",
    "4. Carga de datos en una base de datos SQLite utilizando SQLAlchemy\n",
    "5. Implementación de DAGs (Directed Acyclic Graphs)\n",
    "6. Configuración de logging para seguimiento del proceso\n",
    "\n",
    "Comencemos explorando la estructura del proyecto y los componentes principales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estructura del Proyecto\n",
    "\n",
    "Nuestro proyecto ETL está organizado de la siguiente manera:\n",
    "\n",
    "```\n",
    "notebook_polars_pyspark/\n",
    "├── data/\n",
    "│   └── yellow_tripdata.parquet  # Dataset de taxis de Nueva York\n",
    "├── etl_example/\n",
    "│   ├── __init__.py\n",
    "│   ├── etl_config.py      # Configuración del ETL\n",
    "│   ├── models.py          # Modelos Pydantic para validación\n",
    "│   ├── database.py        # Configuración de SQLAlchemy\n",
    "│   ├── logger.py          # Configuración de logging\n",
    "│   ├── etl_dag.py         # Implementación de DAGs\n",
    "│   ├── output/            # Directorio para la base de datos\n",
    "│   └── logs/              # Directorio para logs\n",
    "└── notebooks/\n",
    "```\n",
    "\n",
    "Vamos a examinar cada componente del ETL en detalle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuración del ETL (etl_config.py)\n",
    "\n",
    "El archivo `etl_config.py` contiene la configuración básica para nuestro ETL, incluyendo rutas de archivos, configuración de la base de datos y parámetros de logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\n",
      "Configuración para el ETL de taxis de Nueva York\n",
      "\"\"\"\n",
      "import os\n",
      "from pathlib import Path\n",
      "\n",
      "# Rutas de archivos\n",
      "BASE_DIR = Path(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
      "DATA_DIR = BASE_DIR / \"data\" / \"processed\"\n",
      "OUTPUT_DIR = BASE_DIR / \"etl_example\" / \"output\"\n",
      "LOG_DIR = BASE_DIR / \"etl_example\" / \"logs\"\n",
      "\n",
      "# Asegurar que los directorios existan\n",
      "OUTPUT_DIR.mkdir(exist_ok=True)\n",
      "LOG_DIR.mkdir(exist_ok=True)\n",
      "\n",
      "# Configuración de la base de datos\n",
      "DB_PATH = OUTPUT_DIR / \"nyc_taxi.db\"\n",
      "DB_URI = f\"sqlite:///{DB_PATH}\"\n",
      "\n",
      "# Configuración de logging\n",
      "LOG_FILE = LOG_DIR / \"etl_process.log\"\n",
      "LOG_LEVEL = \"INFO\"\n",
      "LOG_FORMAT = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
      "\n",
      "# Configuración del dataset\n",
      "TAXI_DATA_FILE = DATA_DIR / \"yellow_tripdata.parquet\"\n",
      "TAXI_DATA_FILE_SMALL = DATA_DIR / \"yellow_tripdata_small.parquet\"\n",
      "\n",
      "# Configuración de procesamiento\n",
      "BATCH_SIZE = 100000  # Número de filas a procesar en cada lote\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar el contenido del archivo etl_config.py\n",
    "from pathlib import Path\n",
    "\n",
    "contenido = Path(\"etl_example/etl_config.py\").read_text(encoding=\"utf-8\")\n",
    "print(contenido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modelos de Datos con Pydantic (models.py)\n",
    "\n",
    "Utilizamos Pydantic para definir modelos de datos con validación estricta de tipos. Esto nos permite asegurar que los datos cumplen con nuestras expectativas antes de cargarlos en la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\n",
      "Modelos de datos para el ETL de taxis de Nueva York usando Pydantic\n",
      "\"\"\"\n",
      "from datetime import datetime\n",
      "from typing import Optional\n",
      "from pydantic import BaseModel, Field, validator\n",
      "\n",
      "class TaxiTrip(BaseModel):\n",
      "    \"\"\"Modelo Pydantic para validar los datos de viajes de taxi\"\"\"\n",
      "    \n",
      "    # Campos de identificación\n",
      "    vendor_id: int = Field(..., description=\"ID del proveedor del taxi\")\n",
      "    \n",
      "    # Campos de tiempo\n",
      "    pickup_datetime: datetime = Field(..., description=\"Fecha y hora de recogida\")\n",
      "    dropoff_datetime: datetime = Field(..., description=\"Fecha y hora de entrega\")\n",
      "    \n",
      "    # Campos de ubicación\n",
      "    pickup_location_id: int = Field(..., description=\"ID de la ubicación de recogida\")\n",
      "    dropoff_location_id: int = Field(..., description=\"ID de la ubicación de entrega\")\n",
      "    \n",
      "    # Campos de pasajeros\n",
      "    passenger_count: Optional[float] = Field(None, description=\"Número de pasajeros\")\n",
      "    \n",
      "    # Campos de distancia\n",
      "    trip_distance: float = Field(..., description=\"Distancia del viaje en millas\")\n",
      "    \n",
      "    # Campos de tarifa\n",
      "    fare_amount: float = Field(..., description=\"Tarifa base en dólares\")\n",
      "    extra: float = Field(..., description=\"Cargos extra\")\n",
      "    mta_tax: float = Field(..., description=\"Impuesto MTA\")\n",
      "    tip_amount: float = Field(..., description=\"Propina en dólares\")\n",
      "    tolls_amount: float = Field(..., description=\"Peajes en dólares\")\n",
      "    improvement_surcharge: float = Field(..., description=\"Recargo por mejora\")\n",
      "    total_amount: float = Field(..., description=\"Monto total en dólares\")\n",
      "    congestion_surcharge: Optional[float] = Field(None, description=\"Recargo por congestión\")\n",
      "    airport_fee: Optional[float] = Field(None, description=\"Tarifa de aeropuerto\")\n",
      "    \n",
      "    # Campos de pago\n",
      "    payment_type: int = Field(..., description=\"Tipo de pago (1=Tarjeta de crédito, 2=Efectivo, etc.)\")\n",
      "    \n",
      "    # Validadores\n",
      "    @validator('trip_distance')\n",
      "    def trip_distance_must_be_positive(cls, v):\n",
      "        if v < 0:\n",
      "            raise ValueError('La distancia del viaje debe ser positiva')\n",
      "        return v\n",
      "    \n",
      "    @validator('fare_amount', 'total_amount')\n",
      "    def amount_must_be_valid(cls, v):\n",
      "        if v < 0:\n",
      "            raise ValueError('Los montos deben ser positivos o cero')\n",
      "        return v\n",
      "    \n",
      "    @validator('dropoff_datetime')\n",
      "    def dropoff_after_pickup(cls, v, values):\n",
      "        if 'pickup_datetime' in values and v < values['pickup_datetime']:\n",
      "            raise ValueError('La fecha de entrega debe ser posterior a la fecha de recogida')\n",
      "        return v\n",
      "    \n",
      "    @validator('passenger_count')\n",
      "    def passenger_count_must_be_valid(cls, v):\n",
      "        if v is not None and (v < 0 or v > 9):\n",
      "            raise ValueError('El número de pasajeros debe estar entre 0 y 9')\n",
      "        return v\n",
      "\n",
      "class Location(BaseModel):\n",
      "    \"\"\"Modelo Pydantic para validar los datos de ubicaciones\"\"\"\n",
      "    \n",
      "    location_id: int = Field(..., description=\"ID único de la ubicación\")\n",
      "    borough: str = Field(..., description=\"Distrito\")\n",
      "    zone: str = Field(..., description=\"Zona\")\n",
      "    service_zone: str = Field(..., description=\"Zona de servicio\")\n",
      "    \n",
      "    @validator('location_id')\n",
      "    def location_id_must_be_positive(cls, v):\n",
      "        if v <= 0:\n",
      "            raise ValueError('El ID de ubicación debe ser positivo')\n",
      "        return v\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar el contenido del archivo models.py\n",
    "contenido = Path(\"etl_example/models.py\").read_text(encoding=\"utf-8\")\n",
    "print(contenido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ventajas de Pydantic para Validación de Datos\n",
    "\n",
    "Pydantic ofrece varias ventajas para la validación de datos en flujos ETL:\n",
    "\n",
    "1. **Validación de tipos en tiempo de ejecución**: Pydantic valida automáticamente los tipos de datos y convierte valores cuando es posible.\n",
    "2. **Validadores personalizados**: Podemos definir funciones de validación personalizadas para reglas de negocio específicas.\n",
    "3. **Documentación integrada**: Los modelos Pydantic son autodocumentados con descripciones de campos.\n",
    "4. **Integración con FastAPI y otras bibliotecas**: Pydantic se integra bien con el ecosistema de Python.\n",
    "5. **Manejo de errores detallado**: Proporciona mensajes de error claros cuando la validación falla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuración de la Base de Datos con SQLAlchemy (database.py)\n",
    "\n",
    "Utilizamos SQLAlchemy para definir el esquema de la base de datos y gestionar las conexiones. SQLAlchemy nos permite trabajar con bases de datos de manera orientada a objetos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\n",
      "Configuración de la base de datos SQLite para el ETL de taxis de Nueva York\n",
      "\"\"\"\n",
      "from sqlalchemy import Column, Integer, Float, String, DateTime, ForeignKey, create_engine\n",
      "from sqlalchemy.ext.declarative import declarative_base\n",
      "from sqlalchemy.orm import relationship, sessionmaker\n",
      "\n",
      "from etl_example.etl_config import DB_URI\n",
      "\n",
      "# Crear la base para los modelos SQLAlchemy\n",
      "Base = declarative_base()\n",
      "\n",
      "class TaxiLocation(Base):\n",
      "    \"\"\"Modelo SQLAlchemy para la tabla de ubicaciones\"\"\"\n",
      "    __tablename__ = 'locations'\n",
      "    \n",
      "    location_id = Column(Integer, primary_key=True)\n",
      "    borough = Column(String, nullable=False)\n",
      "    zone = Column(String, nullable=False)\n",
      "    service_zone = Column(String, nullable=False)\n",
      "    \n",
      "    # Relación con los viajes (pickup)\n",
      "    pickup_trips = relationship(\"TaxiTripRecord\", foreign_keys=\"TaxiTripRecord.pickup_location_id\", back_populates=\"pickup_location\")\n",
      "    # Relación con los viajes (dropoff)\n",
      "    dropoff_trips = relationship(\"TaxiTripRecord\", foreign_keys=\"TaxiTripRecord.dropoff_location_id\", back_populates=\"dropoff_location\")\n",
      "\n",
      "class TaxiTripRecord(Base):\n",
      "    \"\"\"Modelo SQLAlchemy para la tabla de viajes de taxi\"\"\"\n",
      "    __tablename__ = 'taxi_trips'\n",
      "    \n",
      "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
      "    vendor_id = Column(Integer, nullable=False)\n",
      "    \n",
      "    # Campos de tiempo\n",
      "    pickup_datetime = Column(DateTime, nullable=False)\n",
      "    dropoff_datetime = Column(DateTime, nullable=False)\n",
      "    \n",
      "    # Campos de ubicación con relaciones\n",
      "    pickup_location_id = Column(Integer, ForeignKey('locations.location_id'), nullable=False)\n",
      "    dropoff_location_id = Column(Integer, ForeignKey('locations.location_id'), nullable=False)\n",
      "    \n",
      "    # Relaciones\n",
      "    pickup_location = relationship(\"TaxiLocation\", foreign_keys=[pickup_location_id], back_populates=\"pickup_trips\")\n",
      "    dropoff_location = relationship(\"TaxiLocation\", foreign_keys=[dropoff_location_id], back_populates=\"dropoff_trips\")\n",
      "    \n",
      "    # Campos de pasajeros\n",
      "    passenger_count = Column(Float)\n",
      "    \n",
      "    # Campos de distancia\n",
      "    trip_distance = Column(Float, nullable=False)\n",
      "    \n",
      "    # Campos de tarifa\n",
      "    fare_amount = Column(Float, nullable=False)\n",
      "    extra = Column(Float, nullable=False)\n",
      "    mta_tax = Column(Float, nullable=False)\n",
      "    tip_amount = Column(Float, nullable=False)\n",
      "    tolls_amount = Column(Float, nullable=False)\n",
      "    improvement_surcharge = Column(Float, nullable=False)\n",
      "    total_amount = Column(Float, nullable=False)\n",
      "    congestion_surcharge = Column(Float)\n",
      "    airport_fee = Column(Float)\n",
      "    \n",
      "    # Campos de pago\n",
      "    payment_type = Column(Integer, nullable=False)\n",
      "\n",
      "def init_db():\n",
      "    \"\"\"Inicializa la base de datos creando todas las tablas\"\"\"\n",
      "    engine = create_engine(DB_URI)\n",
      "    Base.metadata.create_all(engine)\n",
      "    return engine\n",
      "\n",
      "def get_session():\n",
      "    \"\"\"Crea y devuelve una sesión de SQLAlchemy\"\"\"\n",
      "    engine = create_engine(DB_URI)\n",
      "    Session = sessionmaker(bind=engine)\n",
      "    return Session()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar el contenido del archivo database.py\n",
    "contenido = Path(\"etl_example/database.py\").read_text(encoding=\"utf-8\")\n",
    "print(contenido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ventajas de SQLAlchemy para ETL\n",
    "\n",
    "SQLAlchemy ofrece varias ventajas para los procesos ETL:\n",
    "\n",
    "1. **Abstracción de la base de datos**: Podemos cambiar el motor de base de datos sin modificar el código.\n",
    "2. **Mapeo objeto-relacional (ORM)**: Trabajamos con objetos Python en lugar de SQL directo.\n",
    "3. **Gestión de sesiones**: Manejo eficiente de transacciones y conexiones.\n",
    "4. **Migraciones de esquema**: Facilita la evolución del esquema de la base de datos.\n",
    "5. **Validación a nivel de base de datos**: Complementa la validación de Pydantic con restricciones a nivel de base de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuración de Logging (logger.py)\n",
    "\n",
    "El sistema de logging nos permite seguir el progreso del ETL y diagnosticar problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\n",
      "Configuración del sistema de logging para el ETL de taxis de Nueva York\n",
      "\"\"\"\n",
      "import logging\n",
      "import sys\n",
      "from pathlib import Path\n",
      "\n",
      "from etl_example.etl_config import LOG_FILE, LOG_LEVEL, LOG_FORMAT\n",
      "\n",
      "def setup_logger(name, log_file=LOG_FILE, level=LOG_LEVEL):\n",
      "    \"\"\"Configura y devuelve un logger con el nombre especificado\"\"\"\n",
      "    \n",
      "    # Crear el directorio de logs si no existe\n",
      "    log_dir = Path(log_file).parent\n",
      "    log_dir.mkdir(exist_ok=True)\n",
      "    \n",
      "    # Configurar el logger\n",
      "    logger = logging.getLogger(name)\n",
      "    \n",
      "    # Establecer el nivel de logging\n",
      "    level_obj = getattr(logging, level)\n",
      "    logger.setLevel(level_obj)\n",
      "    \n",
      "    # Crear un manejador para archivo\n",
      "    file_handler = logging.FileHandler(log_file)\n",
      "    file_handler.setLevel(level_obj)\n",
      "    \n",
      "    # Crear un manejador para consola\n",
      "    console_handler = logging.StreamHandler(sys.stdout)\n",
      "    console_handler.setLevel(level_obj)\n",
      "    \n",
      "    # Crear el formato\n",
      "    formatter = logging.Formatter(LOG_FORMAT)\n",
      "    file_handler.setFormatter(formatter)\n",
      "    console_handler.setFormatter(formatter)\n",
      "    \n",
      "    # Agregar los manejadores al logger\n",
      "    logger.addHandler(file_handler)\n",
      "    logger.addHandler(console_handler)\n",
      "    \n",
      "    return logger\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar el contenido del archivo logger.py\n",
    "contenido = Path(\"etl_example/logger.py\").read_text(encoding=\"utf-8\")\n",
    "print(contenido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implementación de DAGs (etl_dag.py)\n",
    "\n",
    "Para implementar DAGs (Directed Acyclic Graphs) que definen el flujo de trabajo del ETL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\n",
      "Implementación de ETL para taxis de Nueva York usando sistema de DAGs personalizado\n",
      "\"\"\"\n",
      "from typing import List, Dict, Any\n",
      "import time\n",
      "\n",
      "import polars as pl\n",
      "from sqlalchemy.orm import Session\n",
      "\n",
      "from etl_example.etl_config import TAXI_DATA_FILE_SMALL, BATCH_SIZE\n",
      "from etl_example.logger import setup_logger\n",
      "from etl_example.models import TaxiTrip\n",
      "from etl_example.database import init_db, get_session, TaxiTripRecord, TaxiLocation\n",
      "from utils import DAG\n",
      "\n",
      "# Configurar el logger\n",
      "logger = setup_logger(\"nyc_taxi_etl\")\n",
      "\n",
      "# Crear el DAG\n",
      "taxi_dag = DAG(\n",
      "    dag_id=\"nyc_taxi_etl\",\n",
      "    description=\"ETL para procesar datos de taxis de Nueva York\"\n",
      ")\n",
      "\n",
      "def extract_taxi_data(file_path: str = str(TAXI_DATA_FILE_SMALL)) -> pl.DataFrame:\n",
      "    \"\"\"Extrae los datos del archivo parquet utilizando Polars\"\"\"\n",
      "    logger.info(f\"Extrayendo datos del archivo: {file_path}\")\n",
      "\n",
      "    try:\n",
      "        # Usar Polars para leer el archivo parquet\n",
      "        df = pl.read_parquet(file_path)\n",
      "\n",
      "        # Renombrar columnas para que coincidan con nuestro modelo\n",
      "        column_mapping = {\n",
      "            \"VendorID\": \"vendor_id\",\n",
      "            \"tpep_pickup_datetime\": \"pickup_datetime\",\n",
      "            \"tpep_dropoff_datetime\": \"dropoff_datetime\",\n",
      "            \"PULocationID\": \"pickup_location_id\",\n",
      "            \"DOLocationID\": \"dropoff_location_id\",\n",
      "            \"passenger_count\": \"passenger_count\",\n",
      "            \"trip_distance\": \"trip_distance\",\n",
      "            \"fare_amount\": \"fare_amount\",\n",
      "            \"extra\": \"extra\",\n",
      "            \"mta_tax\": \"mta_tax\",\n",
      "            \"tip_amount\": \"tip_amount\",\n",
      "            \"tolls_amount\": \"tolls_amount\",\n",
      "            \"improvement_surcharge\": \"improvement_surcharge\",\n",
      "            \"total_amount\": \"total_amount\",\n",
      "            \"congestion_surcharge\": \"congestion_surcharge\",\n",
      "            \"Airport_fee\": \"airport_fee\",\n",
      "            \"payment_type\": \"payment_type\"\n",
      "        }\n",
      "\n",
      "        # Renombrar columnas\n",
      "        for old_name, new_name in column_mapping.items():\n",
      "            if old_name in df.columns:\n",
      "                df = df.rename({old_name: new_name})\n",
      "\n",
      "        logger.info(f\"Datos extraídos exitosamente. Filas: {df.shape[0]}, Columnas: {df.shape[1]}\")\n",
      "        return df\n",
      "\n",
      "    except Exception as e:\n",
      "        logger.error(f\"Error al extraer datos: {str(e)}\")\n",
      "        raise\n",
      "\n",
      "def transform_taxi_data(df: pl.DataFrame) -> pl.DataFrame:\n",
      "    \"\"\"Transforma los datos utilizando Polars\"\"\"\n",
      "    logger.info(\"Iniciando transformación de datos\")\n",
      "\n",
      "    try:\n",
      "        # Filtrar viajes con distancia válida (mayor a 0)\n",
      "        df = df.filter(pl.col(\"trip_distance\") > 0)\n",
      "\n",
      "        # Filtrar viajes con tarifa válida (mayor o igual a 0)\n",
      "        df = df.filter(pl.col(\"fare_amount\") >= 0)\n",
      "\n",
      "        # Calcular la duración del viaje en minutos\n",
      "        df = df.with_columns([\n",
      "            ((pl.col(\"dropoff_datetime\").dt.epoch() - pl.col(\"pickup_datetime\").dt.epoch()) / 60).alias(\"trip_duration_minutes\")\n",
      "        ])\n",
      "\n",
      "        # Filtrar viajes con duración válida (mayor a 0)\n",
      "        df = df.filter(pl.col(\"trip_duration_minutes\") > 0)\n",
      "\n",
      "        # Calcular la velocidad promedio (millas por hora)\n",
      "        df = df.with_columns([\n",
      "            (pl.col(\"trip_distance\") / (pl.col(\"trip_duration_minutes\") / 60)).alias(\"avg_speed_mph\")\n",
      "        ])\n",
      "\n",
      "        # Filtrar velocidades razonables (menos de 100 mph)\n",
      "        df = df.filter(pl.col(\"avg_speed_mph\") < 100)\n",
      "\n",
      "        # Manejar valores nulos\n",
      "        df = df.with_columns([\n",
      "            pl.col(\"passenger_count\").fill_null(1),\n",
      "            pl.col(\"congestion_surcharge\").fill_null(0),\n",
      "            pl.col(\"airport_fee\").fill_null(0)\n",
      "        ])\n",
      "\n",
      "        logger.info(f\"Transformación completada. Filas restantes: {df.shape[0]}\")\n",
      "        return df\n",
      "\n",
      "    except Exception as e:\n",
      "        logger.error(f\"Error en la transformación de datos: {str(e)}\")\n",
      "        raise\n",
      "\n",
      "def validate_taxi_data(df: pl.DataFrame) -> List[Dict[str, Any]]:\n",
      "    \"\"\"Valida los datos usando el modelo Pydantic\"\"\"\n",
      "    logger.info(\"Iniciando validación de datos con Pydantic\")\n",
      "    valid_records = []\n",
      "    error_count = 0\n",
      "\n",
      "    chunk_size = BATCH_SIZE\n",
      "    total_rows = df.height\n",
      "\n",
      "    try:\n",
      "        for start in range(0, total_rows, chunk_size):\n",
      "            chunk = df.slice(start, chunk_size)\n",
      "            records = chunk.to_dicts()\n",
      "\n",
      "            for i, record in enumerate(records, start=start):\n",
      "                try:\n",
      "                    validated_record = TaxiTrip(**record)\n",
      "                    valid_records.append(validated_record.dict())\n",
      "                except Exception as e:\n",
      "                    error_count += 1\n",
      "                    if error_count <= 10:  # Limitar el número de errores registrados\n",
      "                        logger.warning(f\"Error de validación en el registro {i}: {str(e)}\")\n",
      "\n",
      "        logger.info(f\"Validación completada. Registros válidos: {len(valid_records)}, Errores: {error_count}\")\n",
      "        return valid_records\n",
      "\n",
      "    except Exception as e:\n",
      "        logger.error(f\"Error en la validación de datos: {str(e)}\")\n",
      "        raise\n",
      "\n",
      "def load_taxi_data(validated_data: List[Dict[str, Any]]) -> None:\n",
      "    \"\"\"Carga los datos validados en la base de datos\"\"\"\n",
      "    logger.info(\"Iniciando carga de datos en la base de datos\")\n",
      "\n",
      "    try:\n",
      "        # Inicializar la base de datos\n",
      "        init_db()\n",
      "\n",
      "        # Obtener una sesión\n",
      "        session = get_session()\n",
      "\n",
      "        # Procesar en lotes para evitar problemas de memoria\n",
      "        total_records = len(validated_data)\n",
      "        batch_size = min(BATCH_SIZE, total_records)\n",
      "\n",
      "        for i in range(0, total_records, batch_size):\n",
      "            batch = validated_data[i:i+batch_size]\n",
      "\n",
      "            # Crear registros en la base de datos\n",
      "            _load_batch(session, batch)\n",
      "\n",
      "            logger.info(f\"Lote procesado: {i+1} a {min(i+batch_size, total_records)} de {total_records}\")\n",
      "\n",
      "        logger.info(\"Carga de datos completada exitosamente\")\n",
      "\n",
      "    except Exception as e:\n",
      "        logger.error(f\"Error en la carga de datos: {str(e)}\")\n",
      "        raise\n",
      "\n",
      "def _load_batch(session: Session, batch: List[Dict[str, Any]]) -> None:\n",
      "    \"\"\"Carga un lote de registros en la base de datos\"\"\"\n",
      "    try:\n",
      "        # Asegurar que las ubicaciones existan\n",
      "        location_ids = set()\n",
      "        for record in batch:\n",
      "            location_ids.add(record['pickup_location_id'])\n",
      "            location_ids.add(record['dropoff_location_id'])\n",
      "\n",
      "        # Verificar qué ubicaciones ya existen en la base de datos\n",
      "        existing_locations = {loc.location_id for loc in session.query(TaxiLocation.location_id).filter(TaxiLocation.location_id.in_(location_ids)).all()}\n",
      "\n",
      "        # Crear las ubicaciones que no existen\n",
      "        for location_id in location_ids:\n",
      "            if location_id not in existing_locations:\n",
      "                # Crear una ubicación temporal\n",
      "                location = TaxiLocation(\n",
      "                    location_id=location_id,\n",
      "                    borough=\"Unknown\",\n",
      "                    zone=f\"Zone {location_id}\",\n",
      "                    service_zone=\"Unknown\"\n",
      "                )\n",
      "                session.add(location)\n",
      "\n",
      "        # Guardar las ubicaciones\n",
      "        session.commit()\n",
      "\n",
      "        # Crear los registros de viajes\n",
      "        for record in batch:\n",
      "            trip = TaxiTripRecord(\n",
      "                vendor_id=record['vendor_id'],\n",
      "                pickup_datetime=record['pickup_datetime'],\n",
      "                dropoff_datetime=record['dropoff_datetime'],\n",
      "                pickup_location_id=record['pickup_location_id'],\n",
      "                dropoff_location_id=record['dropoff_location_id'],\n",
      "                passenger_count=record['passenger_count'],\n",
      "                trip_distance=record['trip_distance'],\n",
      "                fare_amount=record['fare_amount'],\n",
      "                extra=record['extra'],\n",
      "                mta_tax=record['mta_tax'],\n",
      "                tip_amount=record['tip_amount'],\n",
      "                tolls_amount=record['tolls_amount'],\n",
      "                improvement_surcharge=record['improvement_surcharge'],\n",
      "                total_amount=record['total_amount'],\n",
      "                congestion_surcharge=record['congestion_surcharge'],\n",
      "                airport_fee=record['airport_fee'],\n",
      "                payment_type=record['payment_type']\n",
      "            )\n",
      "            session.add(trip)\n",
      "\n",
      "        # Guardar los viajes\n",
      "        session.commit()\n",
      "\n",
      "    except Exception as e:\n",
      "        session.rollback()\n",
      "        logger.error(f\"Error al cargar lote en la base de datos: {str(e)}\")\n",
      "        raise\n",
      "\n",
      "# Definir las tareas del DAG\n",
      "extract_task = taxi_dag.task(\"extract_data\", extract_taxi_data)\n",
      "transform_task = taxi_dag.task(\"transform_data\", transform_taxi_data)\n",
      "validate_task = taxi_dag.task(\"validate_data\", validate_taxi_data)\n",
      "load_task = taxi_dag.task(\"load_data\", load_taxi_data)\n",
      "\n",
      "# Configurar las dependencias\n",
      "extract_task.set_downstream(transform_task)\n",
      "transform_task.set_downstream(validate_task)\n",
      "validate_task.set_downstream(load_task)\n",
      "\n",
      "# Versión síncrona del flujo ETL\n",
      "def nyc_taxi_etl_flow():\n",
      "    \"\"\"Ejecuta el flujo ETL para taxis de Nueva York de forma secuencial\"\"\"\n",
      "    logger.info(\"Iniciando flujo ETL para datos de taxis de Nueva York\")\n",
      "\n",
      "    # Extraer datos\n",
      "    raw_data = extract_taxi_data()\n",
      "\n",
      "    # Transformar datos\n",
      "    transformed_data = transform_taxi_data(raw_data)\n",
      "\n",
      "    # Validar datos\n",
      "    validated_data = validate_taxi_data(transformed_data)\n",
      "\n",
      "    # Cargar datos\n",
      "    load_taxi_data(validated_data)\n",
      "\n",
      "    logger.info(\"Flujo ETL completado exitosamente\")\n",
      "\n",
      "# Versión asíncrona para ejecutar el DAG completo\n",
      "async def run_taxi_etl_dag():\n",
      "    \"\"\"Ejecuta el DAG de ETL de forma asíncrona\"\"\"\n",
      "    logger.info(\"Iniciando DAG de ETL para taxis de Nueva York\")\n",
      "\n",
      "    try:\n",
      "        results = await taxi_dag.run()\n",
      "        logger.info(\"DAG de ETL completado exitosamente\")\n",
      "        return results\n",
      "    except Exception as e:\n",
      "        logger.error(f\"Error al ejecutar el DAG de ETL: {str(e)}\")\n",
      "        raise\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar el contenido del archivo etl_dag.py\n",
    "contenido = Path(\"etl_example/etl_dag.py\").read_text(encoding=\"utf-8\")\n",
    "print(contenido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecutando el ETL\n",
    "\n",
    "Ahora vamos a ejecutar nuestro ETL y analizar su rendimiento. Primero, importamos los módulos necesarios y configuramos el entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuración completada. La base de datos se creará en: /Users/isaromobru/Desktop/DS102024_/4-DataEngineer/PolarsPySpark/etl_example/output/nyc_taxi.db\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Añadir el directorio raíz al path para poder importar los módulos\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# Importar los módulos del ETL\n",
    "from etl_example.etl_dag import nyc_taxi_etl_flow\n",
    "from etl_example.etl_config import DB_PATH, OUTPUT_DIR\n",
    "\n",
    "# Asegurar que el directorio de salida existe\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Eliminar la base de datos si existe para empezar desde cero\n",
    "if DB_PATH.exists():\n",
    "    DB_PATH.unlink()\n",
    "\n",
    "print(f\"Configuración completada. La base de datos se creará en: {DB_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ejecutamos el flujo ETL y medimos el tiempo que tarda en completarse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:40:40,804 - nyc_taxi_etl - INFO - Iniciando flujo ETL para datos de taxis de Nueva York\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:40:40,804 [INFO] Iniciando flujo ETL para datos de taxis de Nueva York\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:40:40,807 - nyc_taxi_etl - INFO - Extrayendo datos del archivo: /Users/isaromobru/Desktop/DS102024_/4-DataEngineer/PolarsPySpark/data/processed/yellow_tripdata_small.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:40:40,807 [INFO] Extrayendo datos del archivo: /Users/isaromobru/Desktop/DS102024_/4-DataEngineer/PolarsPySpark/data/processed/yellow_tripdata_small.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:40:41,366 - nyc_taxi_etl - INFO - Datos extraídos exitosamente. Filas: 2964624, Columnas: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:40:41,366 [INFO] Datos extraídos exitosamente. Filas: 2964624, Columnas: 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:40:41,367 - nyc_taxi_etl - INFO - Iniciando transformación de datos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:40:41,367 [INFO] Iniciando transformación de datos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:40:43,077 - nyc_taxi_etl - INFO - Transformación completada. Filas restantes: 2870076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:40:43,077 [INFO] Transformación completada. Filas restantes: 2870076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:40:43,081 - nyc_taxi_etl - INFO - Iniciando validación de datos con Pydantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:40:43,081 [INFO] Iniciando validación de datos con Pydantic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:40:43,701 - nyc_taxi_etl - WARNING - Error de validación en el registro 6789: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-4.0, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:40:43,701 [WARNING] Error de validación en el registro 6789: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-4.0, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:40:43,865 - nyc_taxi_etl - WARNING - Error de validación en el registro 15250: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-4.0, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:40:43,865 [WARNING] Error de validación en el registro 15250: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-4.0, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:40:45,182 - nyc_taxi_etl - WARNING - Error de validación en el registro 74129: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-5.75, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:40:45,182 [WARNING] Error de validación en el registro 74129: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-5.75, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:40:47,732 - nyc_taxi_etl - WARNING - Error de validación en el registro 140013: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-3.25, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:40:47,732 [WARNING] Error de validación en el registro 140013: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-3.25, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:40:48,682 - nyc_taxi_etl - WARNING - Error de validación en el registro 172424: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-4.0, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:40:48,682 [WARNING] Error de validación en el registro 172424: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-4.0, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:40:50,558 - nyc_taxi_etl - WARNING - Error de validación en el registro 217612: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-4.0, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:40:50,558 [WARNING] Error de validación en el registro 217612: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-4.0, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:40:50,599 - nyc_taxi_etl - WARNING - Error de validación en el registro 218647: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-1.0, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:40:50,599 [WARNING] Error de validación en el registro 218647: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-1.0, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:40:51,196 - nyc_taxi_etl - WARNING - Error de validación en el registro 241072: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-4.0, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:40:51,196 [WARNING] Error de validación en el registro 241072: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-4.0, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:40:51,722 - nyc_taxi_etl - WARNING - Error de validación en el registro 257483: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-5.75, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:40:51,722 [WARNING] Error de validación en el registro 257483: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-5.75, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:40:51,765 - nyc_taxi_etl - WARNING - Error de validación en el registro 258882: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-4.0, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:40:51,765 [WARNING] Error de validación en el registro 258882: 1 validation error for TaxiTrip\n",
      "total_amount\n",
      "  Value error, Los montos deben ser positivos o cero [type=value_error, input_value=-4.0, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:12,781 - nyc_taxi_etl - INFO - Validación completada. Registros válidos: 2869996, Errores: 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:12,781 [INFO] Validación completada. Registros válidos: 2869996, Errores: 80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:12,812 - nyc_taxi_etl - INFO - Iniciando carga de datos en la base de datos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:12,812 [INFO] Iniciando carga de datos en la base de datos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:36,185 - nyc_taxi_etl - INFO - Lote procesado: 1 a 100000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:36,185 [INFO] Lote procesado: 1 a 100000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:59,461 - nyc_taxi_etl - INFO - Lote procesado: 100001 a 200000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:42:59,461 [INFO] Lote procesado: 100001 a 200000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:43:18,849 - nyc_taxi_etl - INFO - Lote procesado: 200001 a 300000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:43:18,849 [INFO] Lote procesado: 200001 a 300000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:43:41,840 - nyc_taxi_etl - INFO - Lote procesado: 300001 a 400000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:43:41,840 [INFO] Lote procesado: 300001 a 400000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:47:51,498 - nyc_taxi_etl - INFO - Lote procesado: 400001 a 500000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:47:51,498 [INFO] Lote procesado: 400001 a 500000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:48:08,785 - nyc_taxi_etl - INFO - Lote procesado: 500001 a 600000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:48:08,785 [INFO] Lote procesado: 500001 a 600000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:48:24,356 - nyc_taxi_etl - INFO - Lote procesado: 600001 a 700000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:48:24,356 [INFO] Lote procesado: 600001 a 700000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:48:43,900 - nyc_taxi_etl - INFO - Lote procesado: 700001 a 800000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:48:43,900 [INFO] Lote procesado: 700001 a 800000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:49:09,941 - nyc_taxi_etl - INFO - Lote procesado: 800001 a 900000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:49:09,941 [INFO] Lote procesado: 800001 a 900000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:49:36,287 - nyc_taxi_etl - INFO - Lote procesado: 900001 a 1000000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:49:36,287 [INFO] Lote procesado: 900001 a 1000000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:49:54,314 - nyc_taxi_etl - INFO - Lote procesado: 1000001 a 1100000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:49:54,314 [INFO] Lote procesado: 1000001 a 1100000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:50:16,131 - nyc_taxi_etl - INFO - Lote procesado: 1100001 a 1200000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:50:16,131 [INFO] Lote procesado: 1100001 a 1200000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:50:33,056 - nyc_taxi_etl - INFO - Lote procesado: 1200001 a 1300000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:50:33,056 [INFO] Lote procesado: 1200001 a 1300000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:50:50,422 - nyc_taxi_etl - INFO - Lote procesado: 1300001 a 1400000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:50:50,422 [INFO] Lote procesado: 1300001 a 1400000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:51:08,620 - nyc_taxi_etl - INFO - Lote procesado: 1400001 a 1500000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:51:08,620 [INFO] Lote procesado: 1400001 a 1500000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:51:25,518 - nyc_taxi_etl - INFO - Lote procesado: 1500001 a 1600000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:51:25,518 [INFO] Lote procesado: 1500001 a 1600000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:51:41,757 - nyc_taxi_etl - INFO - Lote procesado: 1600001 a 1700000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:51:41,757 [INFO] Lote procesado: 1600001 a 1700000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:51:58,291 - nyc_taxi_etl - INFO - Lote procesado: 1700001 a 1800000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:51:58,291 [INFO] Lote procesado: 1700001 a 1800000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:52:14,582 - nyc_taxi_etl - INFO - Lote procesado: 1800001 a 1900000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:52:14,582 [INFO] Lote procesado: 1800001 a 1900000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:52:30,792 - nyc_taxi_etl - INFO - Lote procesado: 1900001 a 2000000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:52:30,792 [INFO] Lote procesado: 1900001 a 2000000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:52:50,942 - nyc_taxi_etl - INFO - Lote procesado: 2000001 a 2100000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:52:50,942 [INFO] Lote procesado: 2000001 a 2100000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:53:15,547 - nyc_taxi_etl - INFO - Lote procesado: 2100001 a 2200000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:53:15,547 [INFO] Lote procesado: 2100001 a 2200000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:53:34,263 - nyc_taxi_etl - INFO - Lote procesado: 2200001 a 2300000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:53:34,263 [INFO] Lote procesado: 2200001 a 2300000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:53:52,154 - nyc_taxi_etl - INFO - Lote procesado: 2300001 a 2400000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:53:52,154 [INFO] Lote procesado: 2300001 a 2400000 de 2869996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:54:10,225 - nyc_taxi_etl - INFO - Lote procesado: 2400001 a 2500000 de 2869996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:54:10,225 [INFO] Lote procesado: 2400001 a 2500000 de 2869996\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m start_time = time.time()\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Ejecutar el flujo\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mnyc_taxi_etl_flow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m end_time = time.time()\n\u001b[32m      8\u001b[39m execution_time = end_time - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DS102024_/4-DataEngineer/PolarsPySpark/etl_example/etl_dag.py:250\u001b[39m, in \u001b[36mnyc_taxi_etl_flow\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    247\u001b[39m validated_data = validate_taxi_data(transformed_data)\n\u001b[32m    249\u001b[39m \u001b[38;5;66;03m# Cargar datos\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[43mload_taxi_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidated_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mFlujo ETL completado exitosamente\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DS102024_/4-DataEngineer/PolarsPySpark/etl_example/etl_dag.py:156\u001b[39m, in \u001b[36mload_taxi_data\u001b[39m\u001b[34m(validated_data)\u001b[39m\n\u001b[32m    153\u001b[39m     batch = validated_data[i:i+batch_size]\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# Crear registros en la base de datos\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[43m_load_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLote procesado: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m(i+batch_size,\u001b[38;5;250m \u001b[39mtotal_records)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m de \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_records\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    160\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mCarga de datos completada exitosamente\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DS102024_/4-DataEngineer/PolarsPySpark/etl_example/etl_dag.py:195\u001b[39m, in \u001b[36m_load_batch\u001b[39m\u001b[34m(session, batch)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m# Crear los registros de viajes\u001b[39;00m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m     trip = \u001b[43mTaxiTripRecord\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvendor_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvendor_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpickup_datetime\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpickup_datetime\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdropoff_datetime\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdropoff_datetime\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpickup_location_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpickup_location_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdropoff_location_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdropoff_location_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpassenger_count\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpassenger_count\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrip_distance\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrip_distance\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfare_amount\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfare_amount\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mextra\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmta_tax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmta_tax\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtip_amount\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtip_amount\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtolls_amount\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtolls_amount\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimprovement_surcharge\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimprovement_surcharge\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_amount\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtotal_amount\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcongestion_surcharge\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcongestion_surcharge\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mairport_fee\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mairport_fee\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpayment_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpayment_type\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    214\u001b[39m     session.add(trip)\n\u001b[32m    216\u001b[39m \u001b[38;5;66;03m# Guardar los viajes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:4\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DS102024_/4-DataEngineer/PolarsPySpark/.venv/lib/python3.11/site-packages/sqlalchemy/orm/state.py:571\u001b[39m, in \u001b[36mInstanceState._initialize_instance\u001b[39m\u001b[34m(*mixed, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     manager.original_init(*mixed[\u001b[32m1\u001b[39m:], **kwargs)\n\u001b[32m    570\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdispatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43minit_failure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DS102024_/4-DataEngineer/PolarsPySpark/.venv/lib/python3.11/site-packages/sqlalchemy/util/langhelpers.py:146\u001b[39m, in \u001b[36msafe_reraise.__exit__\u001b[39m\u001b[34m(self, type_, value, traceback)\u001b[39m\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m exc_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value.with_traceback(exc_tb)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    148\u001b[39m     \u001b[38;5;28mself\u001b[39m._exc_info = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DS102024_/4-DataEngineer/PolarsPySpark/.venv/lib/python3.11/site-packages/sqlalchemy/orm/state.py:569\u001b[39m, in \u001b[36mInstanceState._initialize_instance\u001b[39m\u001b[34m(*mixed, **kwargs)\u001b[39m\n\u001b[32m    566\u001b[39m manager.dispatch.init(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[32m    568\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m569\u001b[39m     \u001b[43mmanager\u001b[49m\u001b[43m.\u001b[49m\u001b[43moriginal_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmixed\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m    571\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m util.safe_reraise():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DS102024_/4-DataEngineer/PolarsPySpark/.venv/lib/python3.11/site-packages/sqlalchemy/orm/decl_base.py:2178\u001b[39m, in \u001b[36m_declarative_constructor\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m   2174\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(cls_, k):\n\u001b[32m   2175\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   2176\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m is an invalid keyword argument for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % (k, cls_.\u001b[34m__name__\u001b[39m)\n\u001b[32m   2177\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2178\u001b[39m \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, k, kwargs[k])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DS102024_/4-DataEngineer/PolarsPySpark/.venv/lib/python3.11/site-packages/sqlalchemy/orm/attributes.py:536\u001b[39m, in \u001b[36mInstrumentedAttribute.__set__\u001b[39m\u001b[34m(self, instance, value)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;129m@__doc__\u001b[39m.classlevel  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__doc__\u001b[39m(\u001b[38;5;28mcls\u001b[39m) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    534\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__doc__\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__set__\u001b[39m(\u001b[38;5;28mself\u001b[39m, instance: \u001b[38;5;28mobject\u001b[39m, value: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    537\u001b[39m     \u001b[38;5;28mself\u001b[39m.impl.set(\n\u001b[32m    538\u001b[39m         instance_state(instance), instance_dict(instance), value, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    539\u001b[39m     )\n\u001b[32m    541\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__delete__\u001b[39m(\u001b[38;5;28mself\u001b[39m, instance: \u001b[38;5;28mobject\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Ejecutar el flujo ETL y medir el tiempo\n",
    "start_time = time.time()\n",
    "\n",
    "# Ejecutar el flujo\n",
    "nyc_taxi_etl_flow()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"\\nETL completado en {execution_time:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificando los Resultados\n",
    "\n",
    "Vamos a verificar que los datos se hayan cargado correctamente en la base de datos SQLite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de viajes en la base de datos: 2869996\n",
      "Número de ubicaciones en la base de datos: 262\n",
      "\n",
      "Muestra de viajes:\n",
      "shape: (5, 18)\n",
      "┌─────┬───────────┬────────────┬────────────┬───┬────────────┬────────────┬────────────┬───────────┐\n",
      "│ id  ┆ vendor_id ┆ pickup_dat ┆ dropoff_da ┆ … ┆ total_amou ┆ congestion ┆ airport_fe ┆ payment_t │\n",
      "│ --- ┆ ---       ┆ etime      ┆ tetime     ┆   ┆ nt         ┆ _surcharge ┆ e          ┆ ype       │\n",
      "│ i64 ┆ i64       ┆ ---        ┆ ---        ┆   ┆ ---        ┆ ---        ┆ ---        ┆ ---       │\n",
      "│     ┆           ┆ str        ┆ str        ┆   ┆ f64        ┆ f64        ┆ f64        ┆ i64       │\n",
      "╞═════╪═══════════╪════════════╪════════════╪═══╪════════════╪════════════╪════════════╪═══════════╡\n",
      "│ 1   ┆ 2         ┆ 2024-01-01 ┆ 2024-01-01 ┆ … ┆ 22.7       ┆ 2.5        ┆ 0.0        ┆ 2         │\n",
      "│     ┆           ┆ 00:57:55.0 ┆ 01:17:43.0 ┆   ┆            ┆            ┆            ┆           │\n",
      "│     ┆           ┆ 00000      ┆ 00000      ┆   ┆            ┆            ┆            ┆           │\n",
      "│ 2   ┆ 1         ┆ 2024-01-01 ┆ 2024-01-01 ┆ … ┆ 18.75      ┆ 2.5        ┆ 0.0        ┆ 1         │\n",
      "│     ┆           ┆ 00:03:00.0 ┆ 00:09:36.0 ┆   ┆            ┆            ┆            ┆           │\n",
      "│     ┆           ┆ 00000      ┆ 00000      ┆   ┆            ┆            ┆            ┆           │\n",
      "│ 3   ┆ 1         ┆ 2024-01-01 ┆ 2024-01-01 ┆ … ┆ 31.3       ┆ 2.5        ┆ 0.0        ┆ 1         │\n",
      "│     ┆           ┆ 00:17:06.0 ┆ 00:35:01.0 ┆   ┆            ┆            ┆            ┆           │\n",
      "│     ┆           ┆ 00000      ┆ 00000      ┆   ┆            ┆            ┆            ┆           │\n",
      "│ 4   ┆ 1         ┆ 2024-01-01 ┆ 2024-01-01 ┆ … ┆ 17.0       ┆ 2.5        ┆ 0.0        ┆ 1         │\n",
      "│     ┆           ┆ 00:36:38.0 ┆ 00:44:56.0 ┆   ┆            ┆            ┆            ┆           │\n",
      "│     ┆           ┆ 00000      ┆ 00000      ┆   ┆            ┆            ┆            ┆           │\n",
      "│ 5   ┆ 1         ┆ 2024-01-01 ┆ 2024-01-01 ┆ … ┆ 16.1       ┆ 2.5        ┆ 0.0        ┆ 1         │\n",
      "│     ┆           ┆ 00:46:51.0 ┆ 00:52:57.0 ┆   ┆            ┆            ┆            ┆           │\n",
      "│     ┆           ┆ 00000      ┆ 00000      ┆   ┆            ┆            ┆            ┆           │\n",
      "└─────┴───────────┴────────────┴────────────┴───┴────────────┴────────────┴────────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import polars as pl\n",
    "\n",
    "# Conectar a la base de datos\n",
    "conn = sqlite3.connect(str(DB_PATH))\n",
    "\n",
    "# Consultar usando read_database\n",
    "trip_count = pl.read_database(query=\"SELECT COUNT(*) FROM taxi_trips\", connection=conn).item(0, 0)\n",
    "location_count = pl.read_database(query=\"SELECT COUNT(*) FROM locations\", connection=conn).item(0, 0)\n",
    "\n",
    "print(f\"Número de viajes en la base de datos: {trip_count}\")\n",
    "print(f\"Número de ubicaciones en la base de datos: {location_count}\")\n",
    "\n",
    "# Consultar algunos viajes para verificar\n",
    "sample_trips = pl.read_database(query=\"SELECT * FROM taxi_trips LIMIT 5\", connection=conn)\n",
    "\n",
    "print(\"\\nMuestra de viajes:\")\n",
    "print(sample_trips)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparación de Rendimiento: Polars vs Pandas\n",
    "\n",
    "Para demostrar las ventajas de rendimiento de Polars sobre Pandas, vamos a implementar una versión simplificada del mismo proceso ETL utilizando Pandas y comparar los tiempos de ejecución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Benchmark: Pandas vs Polars ===\n",
      "\n",
      "1. Ejecutando ETL con Pandas...\n",
      "Extrayendo datos con Pandas...\n",
      "Extracción completada en 4.52 segundos\n",
      "Transformando datos con Pandas...\n",
      "Transformación completada en 17.54 segundos\n",
      "\n",
      "2. Ejecutando ETL con Polars...\n",
      "Extrayendo datos con Polars...\n",
      "Extracción completada en 1.59 segundos\n",
      "Transformando datos con Polars...\n",
      "Transformación completada en 2.74 segundos\n",
      "\n",
      "=== Resultados del Benchmark ===\n",
      "Filas procesadas: 39703334\n",
      "\n",
      "Tiempos de Pandas:\n",
      "  - Extracción: 4.52 segundos\n",
      "  - Transformación: 17.54 segundos\n",
      "  - Total: 22.07 segundos\n",
      "\n",
      "Tiempos de Polars:\n",
      "  - Extracción: 1.59 segundos\n",
      "  - Transformación: 2.74 segundos\n",
      "  - Total: 4.33 segundos\n",
      "\n",
      "Mejora de rendimiento (Polars vs Pandas):\n",
      "  - Extracción: 2.85x más rápido\n",
      "  - Transformación: 6.40x más rápido\n",
      "  - Total: 5.10x más rápido\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from etl_example.etl_config import TAXI_DATA_FILE\n",
    "\n",
    "def etl_with_pandas():\n",
    "    # Extracción\n",
    "    start_time = time.time()\n",
    "    print(\"Extrayendo datos con Pandas...\")\n",
    "    df_pandas = pd.read_parquet(TAXI_DATA_FILE)\n",
    "    extraction_time = time.time() - start_time\n",
    "    print(f\"Extracción completada en {extraction_time:.2f} segundos\")\n",
    "    \n",
    "    # Transformación\n",
    "    start_time = time.time()\n",
    "    print(\"Transformando datos con Pandas...\")\n",
    "    \n",
    "    # Renombrar columnas para consistencia\n",
    "    column_mapping = {\n",
    "        \"VendorID\": \"vendor_id\",\n",
    "        \"tpep_pickup_datetime\": \"pickup_datetime\",\n",
    "        \"tpep_dropoff_datetime\": \"dropoff_datetime\",\n",
    "        \"PULocationID\": \"pickup_location_id\",\n",
    "        \"DOLocationID\": \"dropoff_location_id\"\n",
    "    }\n",
    "    df_pandas = df_pandas.rename(columns=column_mapping)\n",
    "    \n",
    "    # Filtrar viajes con distancia válida\n",
    "    df_pandas = df_pandas[df_pandas['trip_distance'] > 0]\n",
    "    \n",
    "    # Filtrar viajes con tarifa válida\n",
    "    df_pandas = df_pandas[df_pandas['fare_amount'] >= 0]\n",
    "    \n",
    "    # Calcular la duración del viaje en minutos\n",
    "    df_pandas['trip_duration_minutes'] = (df_pandas['dropoff_datetime'] - df_pandas['pickup_datetime']).dt.total_seconds() / 60\n",
    "    \n",
    "    # Filtrar viajes con duración válida\n",
    "    df_pandas = df_pandas[df_pandas['trip_duration_minutes'] > 0]\n",
    "    \n",
    "    # Calcular la velocidad promedio\n",
    "    df_pandas['avg_speed_mph'] = df_pandas['trip_distance'] / (df_pandas['trip_duration_minutes'] / 60)\n",
    "    \n",
    "    # Filtrar velocidades razonables\n",
    "    df_pandas = df_pandas[df_pandas['avg_speed_mph'] < 100]\n",
    "    \n",
    "    # Manejar valores nulos\n",
    "    df_pandas['passenger_count'] = df_pandas['passenger_count'].fillna(1)\n",
    "    df_pandas['congestion_surcharge'] = df_pandas['congestion_surcharge'].fillna(0)\n",
    "    df_pandas['Airport_fee'] = df_pandas['Airport_fee'].fillna(0)\n",
    "    \n",
    "    transformation_time = time.time() - start_time\n",
    "    print(f\"Transformación completada en {transformation_time:.2f} segundos\")\n",
    "    \n",
    "    return {\n",
    "        \"extraction_time\": extraction_time,\n",
    "        \"transformation_time\": transformation_time,\n",
    "        \"total_time\": extraction_time + transformation_time,\n",
    "        \"row_count\": len(df_pandas)\n",
    "    }\n",
    "\n",
    "def etl_with_polars():\n",
    "    import polars as pl\n",
    "    \n",
    "    # Extracción\n",
    "    start_time = time.time()\n",
    "    print(\"Extrayendo datos con Polars...\")\n",
    "    df_polars = pl.read_parquet(TAXI_DATA_FILE)\n",
    "    extraction_time = time.time() - start_time\n",
    "    print(f\"Extracción completada en {extraction_time:.2f} segundos\")\n",
    "    \n",
    "    # Transformación\n",
    "    start_time = time.time()\n",
    "    print(\"Transformando datos con Polars...\")\n",
    "    \n",
    "    # Renombrar columnas para consistencia\n",
    "    column_mapping = {\n",
    "        \"VendorID\": \"vendor_id\",\n",
    "        \"tpep_pickup_datetime\": \"pickup_datetime\",\n",
    "        \"tpep_dropoff_datetime\": \"dropoff_datetime\",\n",
    "        \"PULocationID\": \"pickup_location_id\",\n",
    "        \"DOLocationID\": \"dropoff_location_id\"\n",
    "    }\n",
    "    for old_name, new_name in column_mapping.items():\n",
    "        if old_name in df_polars.columns:\n",
    "            df_polars = df_polars.rename({old_name: new_name})\n",
    "    \n",
    "    # Filtrar viajes con distancia válida\n",
    "    df_polars = df_polars.filter(pl.col(\"trip_distance\") > 0)\n",
    "    \n",
    "    # Filtrar viajes con tarifa válida\n",
    "    df_polars = df_polars.filter(pl.col(\"fare_amount\") >= 0)\n",
    "    \n",
    "    # Calcular la duración del viaje en minutos\n",
    "    df_polars = df_polars.with_columns([\n",
    "        ((pl.col(\"dropoff_datetime\").dt.epoch() - pl.col(\"pickup_datetime\").dt.epoch()) / 60).alias(\"trip_duration_minutes\")\n",
    "    ])\n",
    "    \n",
    "    # Filtrar viajes con duración válida\n",
    "    df_polars = df_polars.filter(pl.col(\"trip_duration_minutes\") > 0)\n",
    "    \n",
    "    # Calcular la velocidad promedio\n",
    "    df_polars = df_polars.with_columns([\n",
    "        (pl.col(\"trip_distance\") / (pl.col(\"trip_duration_minutes\") / 60)).alias(\"avg_speed_mph\")\n",
    "    ])\n",
    "    \n",
    "    # Filtrar velocidades razonables\n",
    "    df_polars = df_polars.filter(pl.col(\"avg_speed_mph\") < 100)\n",
    "    \n",
    "    # Manejar valores nulos\n",
    "    df_polars = df_polars.with_columns([\n",
    "        pl.col(\"passenger_count\").fill_null(1),\n",
    "        pl.col(\"congestion_surcharge\").fill_null(0),\n",
    "        pl.col(\"Airport_fee\").fill_null(0)\n",
    "    ])\n",
    "    \n",
    "    transformation_time = time.time() - start_time\n",
    "    print(f\"Transformación completada en {transformation_time:.2f} segundos\")\n",
    "    \n",
    "    return {\n",
    "        \"extraction_time\": extraction_time,\n",
    "        \"transformation_time\": transformation_time,\n",
    "        \"total_time\": extraction_time + transformation_time,\n",
    "        \"row_count\": df_polars.shape[0]\n",
    "    }\n",
    "\n",
    "# Ejecutar ambas versiones y comparar\n",
    "print(\"=== Benchmark: Pandas vs Polars ===\")\n",
    "print(\"\\n1. Ejecutando ETL con Pandas...\")\n",
    "pandas_results = etl_with_pandas()\n",
    "\n",
    "print(\"\\n2. Ejecutando ETL con Polars...\")\n",
    "polars_results = etl_with_polars()\n",
    "\n",
    "# Calcular la mejora de rendimiento\n",
    "speedup_extraction = pandas_results[\"extraction_time\"] / polars_results[\"extraction_time\"]\n",
    "speedup_transformation = pandas_results[\"transformation_time\"] / polars_results[\"transformation_time\"]\n",
    "speedup_total = pandas_results[\"total_time\"] / polars_results[\"total_time\"]\n",
    "\n",
    "print(\"\\n=== Resultados del Benchmark ===\")\n",
    "print(f\"Filas procesadas: {pandas_results['row_count']}\")\n",
    "print(\"\\nTiempos de Pandas:\")\n",
    "print(f\"  - Extracción: {pandas_results['extraction_time']:.2f} segundos\")\n",
    "print(f\"  - Transformación: {pandas_results['transformation_time']:.2f} segundos\")\n",
    "print(f\"  - Total: {pandas_results['total_time']:.2f} segundos\")\n",
    "\n",
    "print(\"\\nTiempos de Polars:\")\n",
    "print(f\"  - Extracción: {polars_results['extraction_time']:.2f} segundos\")\n",
    "print(f\"  - Transformación: {polars_results['transformation_time']:.2f} segundos\")\n",
    "print(f\"  - Total: {polars_results['total_time']:.2f} segundos\")\n",
    "\n",
    "print(\"\\nMejora de rendimiento (Polars vs Pandas):\")\n",
    "print(f\"  - Extracción: {speedup_extraction:.2f}x más rápido\")\n",
    "print(f\"  - Transformación: {speedup_transformation:.2f}x más rápido\")\n",
    "print(f\"  - Total: {speedup_total:.2f}x más rápido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ventajas de Polars para ETL\n",
    "\n",
    "Basándonos en la implementación y los resultados del benchmark, podemos destacar las siguientes ventajas de Polars para procesos ETL:\n",
    "\n",
    "1. **Rendimiento superior**: Como hemos visto en el benchmark, Polars es significativamente más rápido que Pandas en operaciones de extracción y transformación.\n",
    "\n",
    "2. **Ejecución perezosa (lazy)**: Polars permite definir un plan de ejecución completo antes de ejecutarlo, lo que permite optimizaciones globales.\n",
    "\n",
    "3. **Paralelismo automático**: Polars aprovecha automáticamente todos los núcleos disponibles sin configuración adicional.\n",
    "\n",
    "4. **Eficiencia de memoria**: Polars consume menos memoria que Pandas para las mismas operaciones.\n",
    "\n",
    "5. **API expresiva**: La API de Polars permite expresar transformaciones complejas de manera concisa y legible.\n",
    "\n",
    "6. **Integración con ecosistema de datos**: Polars se integra bien con formatos como Parquet, CSV, JSON, etc.\n",
    "\n",
    "7. **Consistencia de API**: La API de Polars es más consistente y predecible que la de Pandas.\n",
    "\n",
    "Estas ventajas hacen de Polars una excelente opción para procesos ETL que manejan conjuntos de datos medianos a grandes en una sola máquina."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ventajas de la Arquitectura ETL Implementada\n",
    "\n",
    "Nuestra arquitectura ETL combina varias tecnologías modernas para crear un flujo de trabajo robusto y eficiente:\n",
    "\n",
    "1. **Polars para procesamiento de datos**: Aprovechamos el rendimiento y la expresividad de Polars para las operaciones de extracción y transformación.\n",
    "\n",
    "2. **Pydantic para validación de datos**: Utilizamos Pydantic para asegurar que los datos cumplen con nuestras expectativas antes de cargarlos en la base de datos.\n",
    "\n",
    "3. **SQLAlchemy para acceso a base de datos**: Utilizamos SQLAlchemy para definir el esquema de la base de datos y gestionar las conexiones de manera orientada a objetos.\n",
    "\n",
    "4. **Logging para seguimiento**: Configuramos un sistema de logging para seguir el progreso del ETL y diagnosticar problemas.\n",
    "\n",
    "Esta arquitectura proporciona:\n",
    "\n",
    "- **Modularidad**: Cada componente tiene una responsabilidad clara y puede ser modificado o reemplazado independientemente.\n",
    "- **Escalabilidad**: El diseño permite escalar a conjuntos de datos más grandes y flujos de trabajo más complejos.\n",
    "- **Mantenibilidad**: El código está organizado de manera lógica y sigue buenas prácticas de ingeniería de software.\n",
    "- **Robustez**: La validación de datos y el manejo de errores aseguran que el ETL sea resistente a problemas.\n",
    "- **Observabilidad**: El logging y la monitorización permiten seguir el progreso y diagnosticar problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "En este ejemplo, hemos implementado un ETL completo utilizando Polars para procesar el dataset de taxis de Nueva York. Hemos demostrado las ventajas de Polars sobre Pandas en términos de rendimiento y funcionalidades, y hemos construido una arquitectura ETL robusta y eficiente.\n",
    "\n",
    "Las principales conclusiones son:\n",
    "\n",
    "1. **Polars ofrece un rendimiento significativamente mejor que Pandas** para operaciones ETL, especialmente en conjuntos de datos medianos a grandes.\n",
    "\n",
    "2. **La combinación de Polars, Pydantic, SQLAlchemy y un Orquestador** proporciona una arquitectura ETL robusta, eficiente y mantenible.\n",
    "\n",
    "3. **La validación estricta de tipos con Pydantic** asegura la integridad de los datos antes de cargarlos en la base de datos.\n",
    "\n",
    "4. **La implementación de DAGs con Orquestador** permite definir flujos de trabajo complejos de manera clara y gestionar errores de manera efectiva.\n",
    "\n",
    "5. **El logging y la monitorización** son esenciales para seguir el progreso del ETL y diagnosticar problemas.\n",
    "\n",
    "En la siguiente sección, presentaremos un ejercicio práctico para que los estudiantes implementen su propio ETL utilizando estas tecnologías."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
